{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56443b5a",
   "metadata": {},
   "source": [
    "# more data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3caa4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bd1e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with other APIs\n",
    "tickers = ['META', 'AAPL', 'AMZN', 'NFLX', 'GOOGL', 'MSFT', 'IBM', 'ORCL', 'NVDA', 'INTC']\n",
    "\n",
    "# notes:\n",
    "# Marketaux allows 100 calls per day, 3 articles per call max\n",
    "# Alpha Vantage allows 25 calls per day, 50 articles per call max\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0623a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marketaux\n",
    "mt_base_url = \"https://api.marketaux.com/v1/news/all\"\n",
    "mt_api_key = \"vGc2ngli75vh0B0TLLZBYmLoP9j6JdlIO0xnYU1Z\"\n",
    "\n",
    "def request_news_mt(ticker, start, end):\n",
    "    url = f\"{mt_base_url}?symbols={ticker}&published_after={start}&published_before={end}&api_token={mt_api_key}\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Request Stock Error {resp.status_code}\")\n",
    "        return None\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d28298df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_returns_mt(returned_json):\n",
    "#     print(returned_json)\n",
    "    utc = pytz.utc\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    \n",
    "    main_list = []\n",
    "    relation_list = []\n",
    "\n",
    "    main_cols = ['id', 'publish_time', 'title', 'article_url', \n",
    "                 'ticker', 'publisher', 'description', 'keywords']\n",
    "    \n",
    "    for data in returned_json['data']:\n",
    "        # Parse and convert `time_published` to Eastern Time if it exists\n",
    "        publish_time = data.get('published_at')\n",
    "        if publish_time:\n",
    "            # Parse the \"YYYY-MM-DDTHH:MM:SS.SSSSSSZ\" format and assume it's in UTC\n",
    "            dt_utc = datetime.strptime(publish_time, \"%Y-%m-%dT%H:%M:%S.%fZ\").replace(tzinfo=utc)\n",
    "            publish_time_eastern = dt_utc.astimezone(eastern)\n",
    "            publish_time_str = publish_time_eastern.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            publish_time_str = None\n",
    "            \n",
    "        # Extract main fields\n",
    "        entry = {\n",
    "            'id': data.get('uuid'),\n",
    "            'publish_time': publish_time_str,\n",
    "            'title': data.get('title'),\n",
    "            'article_url': data.get('url'),\n",
    "            'ticker': data['entities'][0].get('symbol') if data.get('entities') else None,\n",
    "            'publisher': data.get('source'),\n",
    "            'description': data.get('description'),\n",
    "            'keywords': data.get('keywords')\n",
    "        }\n",
    "        main_list.append(entry)\n",
    "\n",
    "        # Extract related information for 'relation_list' if there are multiple entities\n",
    "        if 'entities' in data:\n",
    "            for entity in data['entities']:\n",
    "                relation_entry = {\n",
    "                    'news_id': data.get('uuid'),\n",
    "                    'source_ticker': entity.get('symbol'),\n",
    "                    'sentiment_score': entity.get('sentiment_score'),\n",
    "                    'time': entry['publish_time'],\n",
    "                    'ticker': entity.get('ticker'),\n",
    "                    'relevance_score': entity.get('match_score', \"\"),\n",
    "                    'ticker_sentiment_score': entity.get('ticker_sentiment_score', \"\"),\n",
    "                    'ticker_sentiment_label': entity.get('ticker_sentiment_label', \"\")\n",
    "                }\n",
    "                relation_list.append(relation_entry)\n",
    "    \n",
    "    # Convert to DataFrames for further processing or saving\n",
    "    # print(main_list)\n",
    "    main_df = pd.DataFrame(main_list)\n",
    "    relation_df = pd.DataFrame(relation_list)\n",
    "    return main_df, relation_df\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98ae8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = ('2024-09-01', '2024-09-03')\n",
    "\n",
    "for ticker in tickers:\n",
    "    main_df_alpha, relation_df_alpha = filter_returns_mt(request_news_mt(ticker, date[0], date[1]))\n",
    "    main_df_alpha.to_csv(f\"dataset/news_maux/{ticker}_main_{date[0]}_{date[1]}.csv\", index=False)\n",
    "    relation_df_alpha.to_csv(f\"dataset/news_maux/{ticker}_relation_{date[0]}_{date[1]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a813793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df_mt, relation_df_mt = filter_returns_mt(request_news_mt('AAPL', '2024-10-01', '2024-10-03'))\n",
    "\n",
    "# main_df_mt.to_csv(\"dataset/news_maux/AAPL_main_1.csv\", index=False)\n",
    "# relation_df_mt.to_csv(\"dataset/news_maux/AAPL_relation_df_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee3852c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_marketaux = filter_returns_mt(request_news_mt('AAPL', '2024-10-01', '2024-10-05'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "588bec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data_marketaux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8348a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_marketaux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37224c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_raw_m = request_news_mt('AAPL', '2024-10-01', '2024-10-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2737f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_d_1 = request_news_mt('AAPL', '2024-10-01', '2024-10-05')\n",
    "\n",
    "# marketaux_path = \"marketaux.json\"\n",
    "# with open(marketaux_path, \"w\") as f:\n",
    "#     json.dump(temp_d_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "624a1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha Advantage\n",
    "alpha_base_url = \"https://www.alphavantage.co/query\"\n",
    "alpha_api_key = \"SMASMZ5KH2MHZ462\"\n",
    "\n",
    "def convert_time(time_str):\n",
    "    # Define the default hour and minute values\n",
    "    default_hour = 0\n",
    "    default_minute = 0\n",
    "    \n",
    "    # Parse the date string with optional time components\n",
    "    try:\n",
    "        # Try parsing the input string with both date and time\n",
    "        dt = datetime.strptime(time_str, '%Y-%m-%dT%H:%M')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # If time is not provided, parse just the date and add default values\n",
    "            dt = datetime.strptime(time_str, '%Y-%m-%d').replace(hour=default_hour, minute=default_minute)\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Invalid date format. Expected 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM'\")\n",
    "    \n",
    "    # Return the formatted string\n",
    "    return dt.strftime('%Y%m%dT%H%M')\n",
    "\n",
    "def request_news_alpha(ticker, start, end):\n",
    "    time_from = convert_time(start)\n",
    "    time_to = convert_time(end)\n",
    "    url = f\"{alpha_base_url}?function=NEWS_SENTIMENT&tickers={ticker}&time_from={time_from}&time_to={time_to}&apikey={alpha_api_key}\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Request Stock Error {resp.status_code}\")\n",
    "        return None\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1badb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_news_feed_alpha(returned_json, ticker): # json return for this one does not have ticker\n",
    "    main_list = []\n",
    "    relation_list = []\n",
    "    \n",
    "    # Define UTC and Eastern Time zones\n",
    "    utc = pytz.utc\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    \n",
    "    for item in returned_json['feed']:\n",
    "        # Parse and convert `time_published` to Eastern Time if it exists\n",
    "        publish_time = item.get('time_published')\n",
    "        if publish_time:\n",
    "            # Parse the \"YYYYMMDDTHHMMSS\" format and assume it's in UTC\n",
    "            dt_utc = datetime.strptime(publish_time, \"%Y%m%dT%H%M%S\").replace(tzinfo=utc)\n",
    "            publish_time_eastern = dt_utc.astimezone(eastern)\n",
    "            publish_time_str = publish_time_eastern.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            publish_time_str = None\n",
    "\n",
    "        # Extract main data fields\n",
    "        entry = {\n",
    "            'id': item.get('id', \"\"),  # Adjust if 'id' has a different key name in your data\n",
    "            'publish_time': publish_time_str,\n",
    "            'title': item.get('title', \"\"),\n",
    "            'article_url': item.get('url', \"\"),\n",
    "            'ticker': ticker,\n",
    "            'publisher': item.get('source', \"\"),\n",
    "            'description': item.get('summary', \"\"),\n",
    "            'keywords': \",\".join([topic['topic'] for topic in item.get('topics', [])])\n",
    "        }\n",
    "        main_list.append(entry)\n",
    "        \n",
    "        # Extract ticker sentiment details for relation data\n",
    "        if 'ticker_sentiment' in item:\n",
    "            for relation in item['ticker_sentiment']:\n",
    "                relation_entry = {\n",
    "                    'news_id': entry['id'],\n",
    "                    'source_ticker': ticker,\n",
    "                    'time': publish_time_str,\n",
    "                    'ticker': relation.get('ticker'),\n",
    "                    'relevance_score': relation.get('relevance_score', \"\"),\n",
    "                    'ticker_sentiment_score': relation.get('ticker_sentiment_score', \"\"),\n",
    "                    'ticker_sentiment_label': relation.get('ticker_sentiment_label', \"\")\n",
    "                }\n",
    "                relation_list.append(relation_entry)\n",
    "    \n",
    "    # Convert lists to dataframes\n",
    "    main_df = pd.DataFrame(main_list)\n",
    "    relation_df = pd.DataFrame(relation_list)\n",
    "    \n",
    "    return main_df, relation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd98bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = ('2024-10-01', '2024-10-03')\n",
    "\n",
    "for ticker in tickers:\n",
    "    main_df_alpha, relation_df_alpha = process_news_feed_alpha(request_news_alpha(ticker, date[0], date[1]), ticker)\n",
    "    main_df_alpha.to_csv(f\"dataset/news_alpha/{ticker}_main_{date[0]}_{date[1]}.csv\", index=False)\n",
    "    relation_df_alpha.to_csv(f\"dataset/news_alpha/{ticker}_relation_{date[0]}_{date[1]}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date acquired\n",
    "# ('2024-10-01', '2024-10-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01d00dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df_alpha, relation_df_alpha = process_news_feed_alpha(request_news_alpha('AAPL', '2024-10-01', '2024-10-03'), 'AAPL')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e53b0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df_alpha.to_csv(\"dataset/news_alpha/AAPL_main_1.csv\", index=False)\n",
    "# relation_df_alpha.to_csv(\"dataset/news_alpha/AAPL_relation_df_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa40d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_alpha = filter_returns_alpha(request_news_alpha('AAPL', '2024-10-01', '2024-10-03'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77c4f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a4b6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36aa1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_d_2 = request_news_alpha('AAPL', '2024-10-01', '2024-10-05')\n",
    "\n",
    "# alpha_path = \"alpha.json\"\n",
    "# with open(alpha_path, \"w\") as f:\n",
    "#     json.dump(temp_d_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e7d8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_l = [\n",
    "#     ('2023-10-01', '2023-10-05'),\n",
    "#     ('2023-10-01', '2023-10-10'),\n",
    "#     ('2023-10-01', '2023-11-01'),\n",
    "#     ('2023-10-01', '2023-12-30'),\n",
    "#     ('2023-10-01', '2024-06-01'),\n",
    "# ]\n",
    "\n",
    "# for t in time_l:\n",
    "#     dt = filter_returns_alpha(request_news_alpha('AAPL', t[0], t[1]))\n",
    "#     print(len(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b4fdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b10bd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(filter_returns_alpha(request_news_alpha('AAPL', '2023-10-01', '2023-10-04'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08712da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
